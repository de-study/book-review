![history](https://velog.velcdn.com/images/geunwoobaek/post/72c0f656-9d6e-49d4-9fbb-395f81bdff1d/image.png)

## 스파크: RDD의 아래에는 무엇이 있는가
**RDD (Resilient Distributed Dataset)**
- 스파크의 가장 기본적인 데이터 추상화 모델
	- 클러스터의 여러 노드에 흩어져 있는 **데이터들의 집합체**
- RDD는 스파크가 처음 등장했을 때 기존 맵리듀스(MapReduce)의 한계를 극복하기 위해 설계
	- 핵심은 데이터를 디스크가 아닌 **메모리**에 유지하면서도, 시스템에 문제가 생겼을 때 데이터를 잃어버리지 않고 복구할 수 있는 '탄력성(Resilient)'을 갖췄다는 점

**RDD 3가지 사용자 관점 특성**
- 1. **읽기 전용(Immutable)**. 
	- 한 번 만들어진 RDD는 수정할 수 없고, 변형을 가하면 새로운 RDD가 생성. 
- 2. **분산 저장**
	- 거대한 데이터를 조각(Partition)내어 여러 서버에 나눠 저장
- 3. **지연 실행(Lazy Evaluation)**
	- 액션(Action)이 호출되기 전까지는 실제 계산을 미루고 계산 계획만 세운다. 
- 이 기초적인 설계 덕분에 스파크는 대규모 데이터를 빛의 속도로 처리할 수 있는 기반 마련

**RDD의 3가지 물리적인 내부 구현 관점의 핵심 특성**
1. **의존성**
	- 어떤 입력을 필요로 하고 현재의 RDD가 어떻게 만들어지는지 스파크에게 가르쳐준다.
	- 결과를 새로 만들어야 하는 경우, 스파크는 의존성 정보를 참조하고 연산을 다시 반복하여 RDD를 다시 만들 수 있다.
		- 이 특성이 RDD에 유연성 부여
2. **파티션 (지역성 정보 포함)**
	- 스파크에게 작업을 나눠서 이그제큐터들에 분산해 파티션별로 병렬 연산할 수 있는 능력 부여
3. **연산 함수: `Partition → Iterator[T]`**
	- RDD에 저장되는 데이터를 `Iterator[T]` 형태로 연산 함수가 만들어 준다

**원조 모델의 문제**
1. 연산 함수나 연산식 자체가 투명하지 않았다.
	- 사용자가 연산 함수 안에서 무엇을 하는지 스파크가 알 수 없었다.
2. 스파크가 함수에서의 연산이나 표현식을 검사하지 못하다 보니 최적화할 방법이 없었다.
	- 스파크가 할 수 있는 것은 정체를 알 수 없는객체를 바이트 뭉치로 직렬화하는 것
		- 데이터 압축 테크닉 적용 불가능
3. 이런 불투명함 때문에, 스파크가 연산 순서를 재정렬해 효과적인 질의 계획으로 바꾸기 어려웠다.

**원조 모델의 문제 추가 설명**
- **최적화의 부재 (No Optimizer)**
	- **카탈리스트 옵티마이저(Catalyst Optimizer)**의 혜택을 받지 못한다
		- RDD 입장에서는 사용자가 작성한 코드가 블랙박스와 같다. 
		- 내부 로직을 들여다보고 "아, 이 필터를 먼저 적용하면 빠르겠는데?" 같은 최적화를 스스로 수행할 수 없음. 
			- 즉, 개발자가 코드를 비효율적으로 짜면 스파크가 도와줄 방법이 없이 그대로 느리게 실행
- **직렬화 오버헤드와 타입 제한**
	- RDD는 자바나 스칼라의 객체를 그대로 담기 때문에 이를 네트워크로 전송하려면 **직렬화(Serialization)** 과정이 필수적
		- 이 과정에서 메모리 사용량이 급증하고 CPU 자원도 많이 소모 
		- 데이터 구조를 스파크가 알지 못하기 때문에, 데이터프레임(DataFrame)처럼 압축된 바이너리 포맷(Tungsten)을 활용해 메모리를 극도로 아끼는 기술 적용 어려움
- **고된 개발 생산성**
- 비즈니스 로직을 하나하나 함수형 프로그래밍 방식으로 구현해야 해서 코드 양이 많아지고 복잡해진다. 
	- SQL처럼 직관적인 언어로 처리할 수 있는 일을 RDD로는 수십 줄의 코드로 구현해야 할 때가 많다. 
- 그래서 요즘은 성능과 편의성을 모두 잡은 데이터프레임이나 데이터셋(Dataset) API를 주로 쓰고, RDD는 꼭 필요한 특수 상황에서만 사용

## 스파크의 구조 확립
**새롭게 도입한 스파크 구조 확립을 위한 핵심 개념**
1. **데이터 분석을 통해 찾은 일상적인 패턴들을 써서 연산 표현**
	- 이 패턴들은 필터링, 선택, 집합연산, 집계, 평균, 그룹화 같은 고수준 연산으로 변환
	- 명료함과 단순함을 더했다.
2. **DSL에서 이러한 연산 사용하면서 지원언어 (e.g. 파이썬, 자바) 에서 API 사용 가능**
	- 이 연산자들은 스파크에게 데이터로 무엇을 작업하고 싶은지, 결과로 무엇을 원하는지 알려줄 수 있다.
	- 이를 통해, 실행을 위한 효율적인 플랜 작성 가능
3. **SQL 테이블이나 스프레드시트처럼, 지원하는 정형화 데이터 타입을 써서 데이터를 표 형태로 구성 가능**
### 핵심적인 장점과 이득
- 구조를 갖추면 스파크 컴포넌트를 통틀어 더 나은 성능과 공간 효율성 등 많은 이득 얻을 수 있다.
```python
# 저수준의 RDD API를 이용한 예제
spark = SparkSession.builder.appName("DataFrame").getOrCreate()
sc = spark.sparkContext
# (name, age) 형태의 튜플로 된 RDD 생성
dataRDD = sc.parallelize([("Brooke", 20), ("Denny", 31), ("Jules", 30), ("TD", 35), ("Brooke", 25)])
# 집계와 평균을 위한 람다 표현식, map, reduceByKey transformation
ageRDD = (dataRDD
          .map(lambda x: (x[0], (x[1], 1)))
          .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
          .map(lambda x: (x[0], x[1][0] / x[1][1]))
          )
print(ageRDD.collect())
spark.stop()
```
```python
# 고수준 DSL, 데이터프레임 API 사용
from pyspark.sql.functions import avg
spark = SparkSession.builder.appName("DataFrame").getOrCreate()

# 데이터프레임 생성
data_df = spark.createDataFrame([("Brooke", 20), ("Denny", 31), ("Jules", 30), ("TD", 35), ("Brooke", 25)], ['name', 'age'])
# 동일한 이름으로 그룹화하여 나이별로 계산해 평균을 구한다.
avg_df = data_df.groupBy('name').agg(avg('age'))
avg_df.show()

spark.stop()
```
- 고수준 DSL 연산자들과 데이터프레임 API 사용하여 스파크에게 무엇을 할지 알려준다면?
	- 훨씬 더 표현력이 높고 이전 버전보다 간단하다.
	- 스파크는 쿼리를 파악해서 사용자의 의도를 이해할 수 있기 때문에 효과적인 실행을 위해 연산을 최적화하거나 적절하게 재배열 가능
- 읽기가 간단하다는 것 말고도, 스파크의 상위 수준 API는 컴포넌트와 언어 통틀어 일관성 갖고 있다.
	- e.g. 스칼라 코드와 파이썬 코드 똑같은 일을 하면서 형태도 비슷
- 개발자들이 중요시하는 단순성이나 표현력은 상위 수준 구조화 API 위에 구축된 스파크 SQL 엔진 덕분에 가능
	- 정형화 스트리밍에서 쓰거나, MLlib에서 쓰거나 질의를 데이터프레임에 대해 사용하는 경우 언제나 정형화인 데이터 형태로 데이터 프레임에서 변환하고 연산
## 데이터프레임 API
- 스파크 데이터프레임은 구조, 포맷 등 몇몇 특정 연산 등에 있어 판다스 데이터프레임에 영향 받았다.
	- 이름 있는 칼럼과 스키마를 가진 분산 인메모리 테이블처럼 동작
	- 각 컬럼은 특정한 데이터 타입 가질 수 있다.
		- e.g. `integer`, `string`, `array`, `map`, `real`, `date`, `timestamp`
- 구조화된 표 형태로 데이터 시각화 → 이해하기 쉽고, 일반적인 연산 형태에서 행과 열 형태로 작업 편리
- 데이터프레임은 불변성을 지닌다.
	- 스파크는 모든 변경 내역 (lineage) 보관 <br> → 이전 버전 내용 보존한 채로 칼럼 이름, 타입 추가하거나 변경 가능
- 데이터프레임에서 이름 붙은 칼럼과 연관 데이터 타입은 스키마에 선언 가능
### 스키마와 데이터프레임 만들기
- **스키마: 데이터프레임을 위해 칼럼 이름과 연관된 데이터 타입 정의한 것**
	- 스키마는 외부 데이터 소스에서 구조화된 데이터 읽어올 때 빈번하게 쓰인다
- **읽을 때 스키마 가져오는 방식과 달리, 미리 스키마를 정의하는 것의 장점**
	1. 스파크가 데이터타입 추측해야 하는 책임 덜어준다.
	2. 스파크가 스키마 확정하기 위해 파일의 만은 부분 읽어들이려고 별도의 잡을 만드는 것 방지
		- 데이터 파일이 크다면, 시간과 비용 많이 소모
	3. 데이터가 스키마와 맞지 않는 경우, 조기에 문제 발견 가능
- ==데이터 소스에서 큰 파일 읽어야 한다면, 반드시 미리 스키마 지정하자==
#### 스키마를 정의하는 2가지 방법
1. **프로그래밍 스타일로 정의**
	- 스파크 데이터프레임 API를 사용한다.
	```python
		# 프로그래밍 스타일
from pyspark.sql.types import *
schema = StructType([StructField("author", StructType(), False),
                     StructField("title", StringType(), False),
                     StructField("pages", IntegerType(), False)])
	```
2. **DDL (Data Definition Language) 사용**
	```python
	# DDL 사용
schema = "author STRING, title STRING, pages INT"
	```
### 칼럼과 표현식
- 데이터프레임에서 이름이 정해진 칼럼들은 특정한 타입의 필드를 나타내는 개념
	- 개념적으로 판다스나 R에서의 데이터프레임이나 RDBMS 테이블의 칼럼과 유사
	- 스파크, 자바, 파이썬은 모두 칼럼과 연관된 공개 메서드 갖고 있다.
		- 이름으로 컬럼 나열, 계산식 형태의 표현식으로 칼럼 값에 연산 수행 가능
- 데이터프레임의 컬럼 객체는 단독으로 존재할 수 없다.
	- 칼럼은 한 레코드의 로우의 일부분
	- 모든 로우가 합쳐져 하나의 데이터프레임 구성
### 로우
- 스파크에서 하나의 행은 일반적으로 하나 이상의 칼럼 갖고 있는 로우 객체로 표현
- 각 칼럼은 동일한 타입이거나 다른 타입일 수 있다.
- 로우는 스파크의 객체이며 순서가 있는 필드 집합 객체
	- 스파크의 지원 언어들에서 각 필드는 0으로 시작하는 인덱스 갖는다
- 일반적으로 스키마를 미리 지정해서 사용하는 것이 데이터프레임 작성에 훨씬 더 빠르고 효율적
	- 대부분의 파일들은 크기가 크기 때문
### 자주 쓰이는 데이터프레임 작업들
- 우선 구조화된 데이터를 갖고 있는 데이터 소스에서 데이터프레임으로 로드해야 한다.
	- `DataFrameReader`라는 이름의 인터페이스 제공
- 데이터를 `JSON`, `CSV`, `Parquet`, `텍스트`, `Avro`, `ORC` 같은 다양한 포맷의 데이터 소스에서 불러들이기 위해 `DataFrameReader`를 사용
- 특정 포맷의 데이터 소스에 데이터 프레임의 데이터를 써서 내보내기 위해서는 `DataFrameWriter`를 사용
#### `DataFrameReader`와 `DataFrameWriter` 사용하기
```python
# 데이터 불러오기 예제 - 샌프란시스코 소방서 데이터
fire_df = spark.read.csv('sf-fire-calls.csv', header=True)
fire_df.show(5)
```
- `DataFrameWriter` 기본 포맷은 칼럼 지향 포맷인 `parquet`
	- `snappy` 압축을 통해 데이터 압축
- 데이터프레임이 `parquet`로 쓰여졌다면 스키마는 `parquet` 메타데이터의 일부로 보존 가능
	- 이런 경우, 데이터프레임 읽어올 때 수동으로 스키마 적용 불필요
#### 데이터프레임을 `parquet`나 `SQL` 테이블로 저장하기
- 일반적으로 데이터 탐색 → 변환 → `parquet` 포맷이나 `SQL` 테이블로 데이터 저장
#### 프로젝션과 필터
- **프로젝션**
	- 수많은 열(Column) 중에서 우리가 진짜 필요한 정보만 수직적으로 선택해내는 과정
	- 프로젝션은 `select()` 메서드로 수행
		- `withColumn()`이나 `selectExpr()`을 통해 새로운 차원을 창조하기도 한다
			- e.g. 성 + 이름 = 풀네임이라는 새로운 프로젝션 결과 생성
	- **Column Pruning**
		- 수천 개의 열을 가진 데이터셋에서 단 3개의 열만 사용한다면, 프로젝션은 나머지 데이터를 메모리에 올리지 않도록 지시하는 역할 수행
		- 특히 `Parquet`, `ORC` 같은 칼럼 기반 저장 형식을 사용할 때 프로젝션이 효율적
			- 프로젝션을 통해 데이터의 너비를 줄여두면 전송되는 바이트 수가 비약적으로 감소
- **필터**
	- 필터는 특정 조건에 부합하는 행(Row)들만 수평적으로 걸러내는 작업
		- 불필요한 행을 제거하여 연산의 대상이 되는 데이터의 모수를 결정하는 강력한 제어 도구
	- `filter()`, `where()` 메서드 통해 특정 불리언(Boolean) 조건을 만족하는 레코드만 남기는 과정
		- 필터를 사용하면 데이터의 행 수가 줄어들기 때문에, 이후에 발생하는 모든 연산의 복잡도가 선형적으로 감소하는 효과
	- 가급적 파이프라인의 가장 앞단에서 필터를 적용하는 것이 조기 필터링의 핵심 원칙
	- **Predicate Pushdown**
		- 필터 조건을 데이터 저장소(Storage) 레벨까지 내려보내서, 조건에 맞지 않는 데이터는 아예 읽지도 않는 방식
			- e.g. 데이터가 날짜별로 파티셔닝 되어 있다면, 특정 날짜 필터를 거는 순간 스파크는 해당 날짜가 아닌 폴더는 쳐다보지도 않고 건너뜀
		- I/O 오버헤드 극단적으로 감소
#### 칼럼 이름 변경 및 추가 삭제
- 스타일이나 컨벤션 준수, 가독성이나 간결성 위해 특정 칼럼 이름 변경 필요
- `StructField`를 써서 스키마 내에서 원하는 칼럼 이름 지정하면 결과 데이터프레임에서 원하는 칼럼 이름 출력
	- 데이터 소스의 칼럼 이름 무시하고 원하는 이름으로 읽어 온다.
- `withColumnRenamed()` 함수를 통해서도 원하는 이름으로 변경 가능
	- 데이터프레임은 변경 불가 방식으로 동작하기 때문에, 기존 칼럼 이름 갖고 있는 원본 유지한 채로 칼럼 이름이 변경된 새로운 데이터 프레임 받아오게 된다.
#### 집계 연산
- 그루팡-카운팅 패턴은 프로젝션이나 필터링만큼 일상적으로 쓰인다
```python
(fire_ts_df
 .select("CallType")
 .where(col("CallType").isNotNull())
 .groupBy("CallType")
 .count()
 .orderBy("count", ascending = False)
 .show(n = 10, truncate = False))
```
- 자주 혹은 반복적으로 질의할 필요 있는 규모 큰 데이터프레임에서는 캐싱 통해 이득 얻을 수 있다.
- 데이터프레임 API는 `collect()` 함수 제공하지만, 극단적으로 큰 데이터프레임에서는 메모리 부족 에러 (Out-of-Memory, OOM) 발생시킬 수 있어 위험
	- 드라이버에 결과 숫자 하나만 전달하는 `count()`와 달리, `collect()`는 전체 데이터프레임 혹은 데이터세트의 모든 Row 객체 모음 반환
	- 몇 개의 Row 결과 보고 싶다면 최초 n개의 Row 객체만 되돌려주는 `take(n)` 함수 쓰는 것이 좋다.

## 데이터세트 API
![아파치 스파크의 정형화 API](https://velog.velcdn.com/images/geunwoobaek/post/fed2cd40-69fa-46f7-91bb-726de594d7bb/image.png)
- 데이터세트는 정적 타입 (Typed) API와 동적 타입 (Untyped) API 두 가지 특성 모두 가진다
- 스칼라의 데이터프레임은 공용 객체의 모음인 `Dataset[Row]`의 다른 이름
	- `Row`는 서로 다른 타입의 값을 저장할 수 있는 포괄적 JVM 객체
- 데이터세트는 스칼라에서 엄격하게 타입이 정해진 JVM 객체의 집합이며, 이 객체는 자바에서는 클래스라고 볼 수 있다.
### 정적 타입 객체, 동적 타입 객체, 포괄적인 Row
- 스파크가 지원하는 언어들에서 데이터세트는 자바와 스칼라에서 통용
- 데이터프레임은 파이썬과 R에서 사용 가능
	- 파이썬과 R이 컴파일 시 타입의 안전을 보장하는 언어가 아니기 때문
	- 타입은 동적으로 추측되거나 컴파일할 때가 아니라 실행 시 정해진다.
- `Row`는 스파크의 포괄적 객체 타입
	- 배열처럼 인덱스 사용하여 접근 가능
	- 다양한 타입의 값 담을 수 있다
- 정적 객체들은 JVM에서 실제 자바 클래스나 스칼라 클래스가 된다
	- 따라서 데이터세트의 각 아이템들은 곧바로 하나의 JVM 객체가 되어 쓸 수 있다.
### 데이터세트 생성
- 데이터세트 만들 때에도 해당 스키마를 알아야 한다. (데이터 타입 모두 알아야 한다)
### 데이터세트에서 가능한 작업들
- 데이터세트에서 작업은 데이터프레임과 유사하게 `filter()`, `map()`, `groupBy()`, `select()`, `take()` 등이 있다.
- 데이터세트가 사용되는 동안 하부의 스파크 SQL 엔진이 JVM 객체의 생성, 변환, 직렬화, 역직렬화 담당
	- 데이터세트 인코더의 도움 받아 자바의 오프힙 메모리 관리 또한 하게 된다.
## 데이터프레임 vs 데이터세트
- 많은 경우 어느 쪽이든 사용하는 언어에 따라 둘 중 하나가 쓰이지만, 일부 상황에서는 한 쪽이 더 선호되는 경우 있다.
- **사실 실무에서는 거의 데이터프레임 사용.** 
	- 데이터세트는 객체로 변환하는 과정(Serialization)에서 성능을 꽤 잡아먹는다. 
	- 정말 타입 안정성이 생명인 금융권이나 복잡한 백엔드 시스템이 아니라면, 스파크의 최적화 능력을 100% 활용할 수 있는 데이터프레임 선호
### 데이터프레임 (DataFrame) 선택해야 하는 상황
- 데이터프레임은 이름 없는 객체들의 집합으로, 내부적으로 스파크가 데이터의 구조를 완전히 파악하고 있어 최적화에 특화

1. **성능 최적화가 최우선일 때**
	- 데이터프레임은 카탈리스트 옵티마이저가 쿼리를 분석해서 가장 빠른 경로를 찾는다
	- 또한 텅스텐(Tungsten) 엔진을 통해 데이터를 바이너리 형태로 압축 저장하기 때문에 메모리 효율이 압도적. 
	- 파이썬이나 R 사용자라면 무조건 데이터프레임 사용

2. **SQL 중심의 비즈니스 로직을 짤 때**
	- "나이가 20세 이상인 사용자 추출" 같은 단순 필터링이나 집계 연산은 SQL이나 데이터프레임 API로 짜는 게 훨씬 직관적이고 빠르다. 
		- 스파크가 "무슨 일을 할지" 알고 있기 때문에 실행 계획을 세우기가 매우 유리

3. **언어의 장벽을 넘어야 할 때**
	- 데이터프레임은 언어에 구애받지 않는다. 
		- 파이썬(PySpark)에서 짜든, 자바에서 짜든 똑같은 텅스텐 엔진을 거치기 때문에 성능 차이가 거의 없다. 
		- 그래서 데이터 엔지니어링 파이프라인의 표준으로 자리 잡았다.
### 데이터세트(Dataset) 선택해야 하는 상황
- 데이터세트는 '타입이 지정된(Typed)' 객체들의 집합으로, 스칼라나 자바 같은 정적 타입 언어에서만 사용 가능
1. **컴파일 시점에 오류를 잡고 싶을 때 (Type Safety)**
	- 데이터프레임은 없는 칼럼 이름을 적어도 코드를 실행해보기 전까지는 알 수 없다. 
	- 반면 데이터세트는 미리 정의된 클래스(Case Class)를 기반으로 작동하기 때문에, 칼럼 이름이 틀리면 **컴파일 단계**에서 문제가 생긴다. 
		- 대규모 프로젝트에서 치명적인 런타임 에러를 방지
2. **복잡한 객체 지향 로직이 필요할 때**
	- 단순한 SQL 연산으로는 표현하기 힘든 복잡한 비즈니스 로직이나 수학적 모델링을 자바/스칼라 객체 메서드로 구현해야 할 때 데이터세트 효과적. 
		- 데이터를 단순히 표로 보는 게 아니라 특정 클래스의 인스턴스로 다룰 수 있다.
3. **데이터의 형태가 고도로 구조화되어 있을 때**
	- 도메인 모델이 명확하고, 여러 개발자가 협업하면서 데이터의 타입을 엄격하게 맞추어야 하는 환경에서는 데이터세트가 훨씬 안전
## 스파크 SQL과 하부의 엔진
- 프로그래밍 레벨에서 스파크 SQL은 개발자들이 스키마를 가진 정형화 데이터에 ANSI SQL 2003 호환 질의 사용할 수 있게 해준다.
- 스파크 SQL 엔진이 하는 일
	- 스파크 컴포넌트들 통합, 데이터프레임 / 데이터세트가 자바, 스칼라, 파이썬, R 등으로 정형화 데이터 관련 작업 단순화할 수 있게 추상화
	- 아파치 하이브 메타스토어와 테이블에 접근
	- 정형화된 파일 포맷 (e.g. `JSON`, `parquet` 등)에서 스키마와 정형화 데이터 읽고 쓰며 데이터를 임시 테이블로 변환
	- 빠른 데이터 탐색 가능하게 대화형 스파크 SQL 셸 제공
	- 표준 데이터베이스 JDBC/ODBC 커넥터 통해 외부 도구와 연결할 수 있는 중간 역할
	- 최종 실행 위해 최적화된 질의 계획과 JVM 위한 최적화된 코드 생성
![스파크 SQL과 그 스택](https://velog.velcdn.com/images/bsch0111/post/442460b8-2658-4aad-b605-5f54d651703d/image.png)
- 카탈리스트 옵티마이저와 텅스텐 프로젝트가 스파크 SQL의 핵심
	- 상위 수준의 데이터프레임과 데이터세트 API 및 SQL 쿼리 등을 지원
### 카탈리스트 옵티마이저
- 카탈리스트 옵티마이저는 연산 쿼리 받아 실행 계획으로 변환
	1. 분석
	2. 논리적 최적화
	3. 물리 계획 수립
	4. 코드 생성
![스파크 연산의 4단계 여정](https://velog.velcdn.com/images/bsch0111/post/4f117f47-a66d-45cb-a514-52941740a9ba/image.png)
![스파크연산의 4단계 여정](https://velog.velcdn.com/images/yunseo00/post/5735f10a-9199-4e23-b9d0-5ae2c919f0c0/image.png)
#### 1단계: 분석
- 스파크 SQL 엔진은 SQL이나 데이터프레임 쿼리 위한 추상 문법 트리 생성으로 시작
- 초기 단계에서는 칼럼, 데이터 타입, 함수, 테이블, 데이터베이스 이름 목록 갖고 있는 스파크 SQL의 프로그래밍 인터페이스인 `Catalog` 객체로 접근하여 가져온다
- 이 과정 성공하면 다음 단계로 쿼리가 넘어간다.
#### 2단계: 논리적 최적화
- 내부적으로 두 가지 단계로 이루어진다
	- 1. 카탈리스트 옵티마이저는 표준적인 규칙 기반으로 하는 최적화 접근 방식 적용하면서 먼저 여러 계획 수립
	2. 비용 기반 옵티마이저 (Cost Based Optimizer, CBO)를 써서 각 계획에 비용 책정
		- 이 계획들은 연산 트리로 배열
			- e.g. 조건절 하부 배치, 칼럼 걸러내기, 불리언 연산 단순화 포함
- 논리 계획은 물리 계획 수립의 입력 데이터
#### 3단계: 물리 계획 수립
- 스파크 SQL은 스파크 실행 엔진에서 선택된 논리 계획을 바탕으로 대응되는 물리적 연산자 사용하여 최적화된 물리 계획 생성
#### 4단계: 코드 생성
- 쿼리 최적화의 마지막 단계는 각 머신에서 실행한 효율적인 자바 바이트 코드 생성하는 것 포함
- 스파크는 실행 속도를 높이기 위한 코드 생성 위해 최신 컴파일러 기술 사용 가능
	- 스파크 SQL가 메모리에 올라와 있는 데이터 집합 다루기 때문 (스파크가 컴파일러처럼 동작)
		- 포괄 (Whole-stage) 코드 생성을 가능하게 하는 프로젝트 텅스텐이 여기서 역할을 하게 된다.
- 포괄 코드 생성: 전체 쿼리를 하나의 함수로 합치면서 가상 함수 호출이나 중간 데이터 위한 CPU 레지스터 사용을 없애버린다
	- 물리적 쿼리 최적화 단계
	- 스파크 2.0에서 도입된 2세대 텅스텐 엔진은 이 접근 방식을 최종 실행 시 콤팩트한 RDD 코드를 생성하는 것에 사용
		- 이러한 간결한 전략 통해 CPU 효율과 성능 극적으로 높였다.