# 1. 아파치 스파크 다운로드

- spark 2.2.2 릴리스 이후 PyPI 저장소로부터 Pyspark를 설치할 수 있음
    - 파이썬으로만 프로그래밍한다면 단순히  `pip install pyspark` 로 다운로드
    - 추가적인 라이브러리도 설치 가능: `pip install pyspark[sql, ml, mllib]`

## 1.1 스파크의 디렉터리와 파일들

- bin: 스파크 셸들을 포함해서(spark-sql, pyspark, spark-shell) 스파크와 상호 작용할 수 있는 대부분의 스크립트가 위치
- sbin: 이 디렉터리의 대부분의 스크립트가 다양한 배포 모드에서 **클러스터의 스파크 컴포넌트들을 시작하고 중지**하기 위한 관리 목적
- kubernetes: spark 2.4 릴리스부터 쿠버네티스 클러스터에서 쓰는 스파크를 위한 도커 이미지 제작을 할 수 있는 Dockerfile들이 위치
- data: MLlib, 정형화 프로그래밍, GraphX 등에서 입력으로 사용되는 `*.txt` 파일이 위치

<br>

# 2. 스칼라 혹은 파이스파크 셸 사용

- spark는 대화형 셸 형태로 동작
- pypark, spark-shell, spark-sql, sparkR의 네 가지 인터프리터들이 포함되어 있어서 일회성 데이터 분석이 가능

## 2.1 Spark 시작

```sql
# pyspark 시작
$ cd bin
$ ./pyspark

# spark-shell 시작
$ cd bin
$ ./spark-shell
```

## 2.2 로컬 머신 사용하기

- 스파크 연산들은 작업으로 표현
- 작업들은 태스크라고 불리는 저수준 RDD 바이트 코드로 변환 → 실행을 위해 스파크의 이그제큐터들에 분산
- 스파크 2.x 이후로 RDD는 하위 수준 API로 조정

### 💡 RDD와 상위 수준 정형화 API(DataFrame, Dataset)

1. RDD (Resilient Distributed Dataset)
    - RDD는 Spark의 가장 기초적인 추상화 모델
    - 데이터 성격: RDD 데이터의 스키마를 Spark가 이해하지 못함
    - 제어 방식: 개발자가 데이터 처리를 위해 구체적인 연산 로직(어떻게 자르고 합칠지)를 직접 작성해야 하는 저수준 제어 방식
    - 최적화: Spark 엔진이 데이터 내부 구조를 모르기 때문에 사용자가 작성한 코드를 스스로 최적화하기 어려움 → 성능은 전적으로 사용자의 코드 효율성에 의존
2. 상위 수준 정형화 API (DataFrame, Dataset)
    - DataFrame과 Dataset은 RDD 위에서 구축된 고수준의 추상화 모델
    - 데이터 성격: 스키마를 가지고 있어 Spark가 데이터의 구조를 명확히 이해
    - 제어 방식: 어떻게 연산할지 보다 무엇을 할지에 집중하는 선언적 방식(SQL 쿼리 작성과 유사)
    - 최적화: Catalyst Optimizer와 텅스턴(Tungsten) 실행 엔진이 사용자의 코드를 분석하여 가장 효율적인 실행 계획을 자동으로 수립

<br>

# 3. 스파크 애플리케이션 개념의 이해

## 3.1 용어 정리

1. 애플리케이션
    - 사용자가 직접 자바, 스칼라, 파이썬 파일 등을 작성하여 `spark-submit`으로 제출하는 독립된 프로그램
    - 드라이버 프로그램과 클러스터의 실행기로 이루어짐
2. SparkSession
    - Spark 코어 기능들과 상호 작용할 수 있는 진입점을 제공하며 그 API로 프로그래밍을 할 수 있게 해주는 객체
    - 스파크 쉘에서 스파크 드라이버는 기본적으로 SparkSession을 제공하지만 스파크 애플리케이션에서는 사용자가 SparkSession 객체를 생성해서 사용해야 함
3. 잡(job)
    - 스파크 액션(action: save(), collect())에 대한 응답으로 생성되는 여러 태스크(task)로 이루어진 병렬 연산
4. 스테이지(stage)
    - 각 잡은 스테이지라 불리는 서로 의존성을 가지는 다수의 태스크 모음으로 나뉨
5. 태스크(task)
    - 스파크 이그제큐터로 보내지는 작업 실행의 가장 기본적인 단위

## 3.2 스파크 애플리케이션과 SparkSession

### 3.2.1 스파크 잡

- 사용자가 `count()`, `save()`, `show()`와 같은 **액션(action) 명령을 내리는 순간 스파크는 하나의 잡(job)을 생성**
- 드라이버는 스파크 애플리케이션을 하나 이상의 스파크 잡으로 변환
    - 하나의 애플리케시션에 여러 액션 명령 → 여러 개의 잡 생성(**1 액션 = 1 잡**)
- 본질적으로 잡이 스파크의 실행 계획이 되며 이 DAG 그래프에서 각각의 노드는 하나 이상의 스파크 스테이지에 해당
    - **실행 계획(DAG: Directed Acyclic Graph)**: 스파크가 작업을 시작하기 전에 데이터를 어떻게 변형할지 그리는 그림 → 순환하지 않는 화살표 그래프
    - 스파크 드라이버는 잡 하나를 분석해서 ‘어떻게 하면 데이터를 덜 옮기고 빠르게 끝낼까’를 고민하며 DAG 생성
    - 실행 계획은 데이터 셔플(네트워크 이동)이 발생하는 지점을 기준으로 끊어서 관리 → 끊어진 조각 하나 하나가 스테이지가 됌

### 3.2.2 스파크 스테이지

- 하나의 잡은 여러 개의 스테이지로 나뉨
- 스테이지를 나누는 가장 중요한 기준은 **데이터 셔플(shuffle)**
- 어떤 작업이 연속적으로 또는 병렬적으로 수행되는지에 맞춰 스테이지에 해당하는 DAG 노드가 생성
- 스파크 이그제큐터끼리의 데이터 전송이 이루어지는 연산(셔플 연산) 범위 경계 위에서 스테이지가 결정되기도 함

### 3.2.3 스파크 태스크

- 각 스테이지는 최소 실행 단위이며 스파크 이그제큐터들 위에서 연합 실행되는 스파크 태스크들로 이루어짐
- 각 태스크는 개별 CPU 코어에 할당되고 데이터의 개별 파티션을 갖고 작업**(1태스크 = 1코어 = 1 파티션)**
- 예시: A 스테이지에서 처리해야 할 파티션 100개, 사용 가능한 전체 CPU 코어 10개
     → 스파크는 10개씩 10번에 걸쳐 태스크 실행
    

# 4. 트랜스포메이션, 지연 평가, 액션

- 분산 데이터의 스파크 연산은 트랜스포메이션(transformation)과 액션(action)으로 구분

## 4.1 트랜스포메이션

- 원본 데이터를 수정하지 않고 하나의 스파크 데이터 프레임을 새로운 데이터 프레임으로 변형(transform)
    - `select()` 나 `filter()` 같은 연산은 원본 데이터 프레임을 수정하지 않고 새로운 데이터 프레임으로 연산 결과를 만들어 되돌려 줌

### 4.1.1 좁은/넓은 트랜스포메이션

- 좁은 트랜스포메이션(narrow): 하나의 입력 파티션을 연산하여 하나의 결과 파티션을 내놓는 트랜스포메이션 - `filter()`, `contains()`
- 넓은 트랜스포메이션(wide): 다른 파티션으로부터 데이터를 읽어 들여서 합치고 디스크에 쓰는 등의 일을 처리 - `groupBy()`, `orderBy()`

## 4.2 지연 평가

- 모든 트랜스포메이션은 뒤늦게 평가 → 결과가 즉시 계산되는 것이 아니라 계보(lineage)의 형태로 기록
- 기록된 리니지는 실행 계획에서 후반쯤에 스파크가 더 효율적으로 실행될 수 있도록 최적화
- 지연 평가는 액션이 실행되는 시점이나 데이터에 실제 접근하는 시점(디스크에서 읽거나 쓰는 시점)까지 실제 실행을 미루는 스파크의 전략
- 지연 평가는 스파크가 사용자의 연계된 트랜스포메이션들을 살펴봄으로써 쿼리 최적화를 가능하게 함
- 리니지와 데이터 불변성은 장애에 대한 데이터 내구성을 제공
    - 스파크는 리니지에 각 트랜스포메션 기록
    - 데이터 프레임들은 트랜스포메이션을 거치는 동안 변하지 않음
    - 단순히 기록된 리니지를 재실행하는 것만으로도 원복 가능

## 4.3 액션

- 하나의 액션은 모든 기록된 트랜스포메이션의 지연 연산을 발동
